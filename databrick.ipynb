{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e07be1ee-48d0-471d-9225-d749c705ae9c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storageAccountName = \"casestudy02\"\n",
    "appID = \"486642b5-cbc7-41d2-b032-79f54047c5b4\"\n",
    "secret = \"qeq8Q~dR4xj~JLgrLEOW-Iai~28VNA~A3yZjjcTU\"\n",
    "fileSystemName = \"input\"\n",
    "tenantID = \"96330ec5-4405-49df-8fef-81647408f9f8\"\n",
    "\n",
    "spark.conf.set(\"fs.azure.account.auth.type.\" + storageAccountName + \".dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(\"fs.azure.account.oauth.provider.type.\" + storageAccountName + \".dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.id.\" + storageAccountName + \".dfs.core.windows.net\", \"\" + appID + \"\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.secret.\" + storageAccountName + \".dfs.core.windows.net\", \"\" + secret + \"\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.\" + storageAccountName + \".dfs.core.windows.net\", \"https://login.microsoftonline.com/\" + tenantID + \"/oauth2/token\")\n",
    "spark.conf.set(\"fs.azure.createRemoteFileSystemDuringInitialization\", \"true\")\n",
    "dbutils.fs.ls(\"abfss://\" + fileSystemName  + \"@\" + storageAccountName + \".dfs.core.windows.net/\")\n",
    "spark.conf.set(\"fs.azure.createRemoteFileSystemDuringInitialization\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f79c241-0c80-4390-bbce-15cb31f123ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">neww: org.apache.spark.sql.DataFrame = [transaction_id: int, product_id: int ... 2 more fields]\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">neww: org.apache.spark.sql.DataFrame = [transaction_id: int, product_id: int ... 2 more fields]\n</div>",
       "datasetInfos": [
        {
         "name": "neww",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "transaction_id",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "product_id",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "sales_amount",
            "nullable": true,
            "type": "double"
           },
           {
            "metadata": {
             "__detected_date_formats": "yyyy-M-d"
            },
            "name": "sales_date",
            "nullable": true,
            "type": "date"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "org.apache.spark.sql.DataFrame"
        }
       ],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "val neww = spark.read.format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .load(\"abfss://input@casestudy02.dfs.core.windows.net/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ab834de-3d45-4579-9d98-306adb056565",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configure JDBC URL\n",
    "jdbcHostname = \"sqlserver://casestudysynapse.sql.azuresynapse.net\"\n",
    "jdbcPort = \"1433\"\n",
    "jdbcDatabase = \"case\"\n",
    "jdbcUsername = \"User\"\n",
    "jdbcPassword = \"Admin@123\"\n",
    "\n",
    "# Connection properties\n",
    "connectionProperties = {\n",
    "    \"user\" : jdbcUsername,\n",
    "    \"password\" : jdbcPassword,\n",
    "    \"driver\" : \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}\n",
    "\n",
    "jdbcUrl = f\"jdbc:sqlserver://casestudysynapse.sql.azuresynapse.net:1433;database=case study pool;user=undefined@casestudysynapse;password=Admin@123;encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.sql.azuresynapse.net;loginTimeout=30;\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a1cffa1-f595-4e5f-9132-68218897db8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import IntegerType, DoubleType, DateType\n",
    "\n",
    "# Read CSV files from Azure Data Lake Storage\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"abfss://input@casestudy02.dfs.core.windows.net/*.csv\")\n",
    "\n",
    "# Data cleaning and transformation\n",
    "df = df.withColumn(\"transaction_id\", F.col(\"transaction_id\").cast(IntegerType()))\n",
    "df = df.withColumn(\"product_id\", F.col(\"product_id\").cast(IntegerType()))\n",
    "df = df.withColumn(\"sales_amount\", F.col(\"sales_amount\").cast(DoubleType()))\n",
    "df = df.withColumn(\"sales_date\", F.to_date(F.col(\"sales_date\"), \"yyyy-MM-dd\").cast(DateType()))\n",
    "\n",
    "# Aggregate sales data\n",
    "agg_df = df.groupBy(\"product_id\").agg(F.sum(\"sales_amount\").alias(\"total_sales\"))\n",
    "\n",
    "# Write transformed data back to Azure Synapse Analytics\n",
    "jdbcUrl = \"jdbc:sqlserver://casestudysynapse.sql.azuresynapse.net:1433;database=case study pool;user=undefined@casestudysynapse;password=Admin@123;encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.sql.azuresynapse.net;loginTimeout=30\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2fa5ba1-c359-4c39-8ee5-91e58a9223c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "agg_df.write.jdbc(url=jdbcUrl, table=\"Final1\", mode=\"overwrite\", properties=connectionProperties)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "databrick",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
